{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 安装项目所需的第三方库"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install paddlepaddle==2.3.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install parl -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install visualdl -i https://mirror.baidu.com/pypi/simple\n",
    "!pip install gym==0.18.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARL 强化学习 框架，架构图"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/5cb6ac79d8ee4ad9b73d53063789c5c28d2ec36b499d47ef930ebf2facf64beb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train.py 代码及公式介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### calc_reward_to_go 函数 介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 基于 蒙特卡罗策略梯度，计算该episode每个步骤的未来总回报（奖励）\n",
    "def calc_reward_to_go(reward_list, gamma=1.0):\n",
    "    for i in range(len(reward_list) - 2, -1, -1):\n",
    "        # G_i = r_i + γ·G_i+1\n",
    "        reward_list[i] += gamma * reward_list[i + 1]  # Gt\n",
    "    return np.array(reward_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "该episode每个步骤的未来总回报（奖励）的计算公式如下：<br>\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "G_t &=\\sum_{k=t+1}^{T} \\gamma^{k-t-1}r_k\\\\ &= r_{t+1}+\\gamma G_{t+1}\n",
    "\\end{split}\n",
    "\\end{equation}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### calc_reward_to_go 函数 举例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该episode每个步骤的未来总回报（奖励）:\n",
      " [21. 20. 18. 15. 11.  6.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建 episode 回报（奖励）列表\n",
    "# 代表该episode每个步骤的回报（奖励）\n",
    "reward_list = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# 计算该episode每个步骤的未来总回报（奖励）\n",
    "calc_reward_list = calc_reward_to_go(reward_list=reward_list, gamma=1.0)\n",
    "\n",
    "print('该episode每个步骤的未来总回报（奖励）:\\n', calc_reward_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其中: <br>\n",
    "第一步的未来总回报（奖励）为 1+2+3+4+5+6 = 21 <br>\n",
    "第二步的未来总回报（奖励）为 2+3+4+5+6 = 20 <br>\n",
    "第三步的未来总回报（奖励）为 3+4+5+6 = 18 <br>\n",
    "以此类推。。。。。 <br>\n",
    "\n",
    "实现上，使用 动态规划 的思路\n",
    "先 计算 第五步的未来总回报（奖励）:<br>\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "TotalReward_5=r_5+r_6=5+6=11\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "再 计算 第四步的未来总回报（奖励）:<br>\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "TotalReward_4&=r_4+r_5+r_6\\\\&=4+5+6\\\\&=r_4+TotalReward_5\\\\&=15\n",
    "\\end{split}\n",
    "\\end{equation}$<br>\n",
    "以此类推。。。。。 <br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## agent.py 代码及公式介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### sample 函数 介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 根据输入观测，采样输出的离散动作，带探索\n",
    "    def sample(self, obs):\n",
    "        \"\"\" 根据观测值 obs 采样（带探索）一个动作\n",
    "        \"\"\"\n",
    "        # 将 观测obs 转换为 Tensor张量\n",
    "        obs = paddle.to_tensor(obs, dtype='float32')\n",
    "        # 根据算法，返回每个离散动作选择的概率\n",
    "        prob = self.alg.predict(obs)\n",
    "        # 将 动作概率prob 转换为 Numpy数组\n",
    "        prob = prob.numpy()\n",
    "\n",
    "        # 根据动作概率选取动作\n",
    "        # 从 离散动作中，根据 每个离散动作的动作概率prob，随机选取 1 个动作\n",
    "        act = np.random.choice(len(prob), 1, p=prob)[0]\n",
    "        return act"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "numpy.random.choice(a, size=None, replace=True, p=None)\n",
    "\n",
    "用途：\n",
    "从a(一维数据)中随机抽取数字，返回指定大小(size)的数组。\n",
    "replace:True表示可以取相同数字，False表示不可以取相同数字。\n",
    "数组p：与数组a相对应，表示取数组a中每个元素的概率，默认为选取每个元素的概率相同。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### sample 函数 举例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机选取到的动作为： 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 动作概率prob，假设有 3 个 离散动作\n",
    "# 对应 每个动作 的 概率，概率总和 为 1.0\n",
    "prob = np.array([0.4, 0.5, 0.1])\n",
    "\n",
    "# 根据动作概率选取动作\n",
    "# 从 离散动作中，根据 每个离散动作的动作概率prob，随机选取 1 个动作\n",
    "act = np.random.choice(len(prob), 1, p=prob)[0]\n",
    "\n",
    "print(\"随机选取到的动作为：\", act)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predict 函数 介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 根据输入观测，预测输出最优的离散动作\n",
    "    def predict(self, obs):\n",
    "        \"\"\" 根据观测值 obs 选择最优动作\n",
    "        \"\"\"\n",
    "        # 将 观测obs 转换为 Tensor张量\n",
    "        obs = paddle.to_tensor(obs, dtype='float32')\n",
    "        # 根据算法，返回每个离散动作选择的概率\n",
    "        prob = self.alg.predict(obs)\n",
    "        # 根据动作概率选择概率最高的动作\n",
    "        # 从 离散动作中，根据 每个离散动作的动作概率prob，选取概率最高的 1个 最优动作\n",
    "        act = prob.argmax().numpy()[0]\n",
    "        return act"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "paddle.argmax(x, axis=None, keepdim=False, dtype='int64', name=None) <br> \n",
    "<br>用途：\n",
    "返回 沿 axis 计算输入 x 的最大元素的索引。\n",
    "axis 默认值为None, 将会对输入的 x 进行平铺展开，返回最大值的索引。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predict 函数 举例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选取到的最优动作为： 1\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "# 动作概率prob，假设有 3 个 离散动作\n",
    "# 对应 一个观测下，每个动作 的 概率，概率总和 为 1.0\n",
    "prob = paddle.to_tensor([0.4, 0.5, 0.1], dtype='float32')\n",
    "\n",
    "# 根据动作概率选取动作\n",
    "# 从 离散动作中，根据 每个离散动作的动作概率prob，选取概率最高的 1个 最优动作\n",
    "act = prob.argmax().numpy()[0]\n",
    "\n",
    "print(\"选取到的最优动作为：\", act)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### learn 函数 介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 智能体进行策略学习（回合更新）\n",
    "    def learn(self, obs, act, reward):\n",
    "        \"\"\" 根据训练数据更新一次模型参数\n",
    "        \"\"\"\n",
    "        # act为动作数组，添加维度 ---> [act]\n",
    "        act = np.expand_dims(act, axis=-1)\n",
    "        # 回报（奖励）数组，添加维度 ---> [reward]\n",
    "        reward = np.expand_dims(reward, axis=-1)\n",
    "\n",
    "        # 将 观测obs数组 转换为 Tensor张量\n",
    "        obs = paddle.to_tensor(obs, dtype='float32')\n",
    "        # 将 动作act数组 转换为 Tensor张量，离散动作，以 整数 表示\n",
    "        act = paddle.to_tensor(act, dtype='int32')\n",
    "        # 将 回报（奖励）reward数组 转换为 Tensor张量\n",
    "        reward = paddle.to_tensor(reward, dtype='float32')\n",
    "\n",
    "        # 根据算法返回损失函数值\n",
    "        loss = self.alg.learn(obs, act, reward)\n",
    "        return loss.numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "numpy.expand_dims(a, axis)\n",
    "\n",
    "用途：\n",
    "从a(数据)中插入一个新轴，该轴将出现在展开的数组形状中的轴位置。\n",
    "轴axis：放置新轴（或多个轴）的扩展轴中的位置。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### learn 函数 举例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作数组为： (6,)\n",
      "添加维度后的动作数组为：\n",
      " [[0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "添加维度后的动作数组维度为：\n",
      " (6, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建 act动作数组\n",
    "act = np.array([0, 1, 2, 1, 1, 0])\n",
    "\n",
    "# act为动作数组，添加维度 ---> [act]\n",
    "act_new = np.expand_dims(act, axis=-1)\n",
    "\n",
    "print('动作数组为：', act.shape)\n",
    "print('添加维度后的动作数组为：\\n', act_new)\n",
    "print('添加维度后的动作数组维度为：\\n', act_new.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## algorithm.py 代码及公式介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### learn 函数 介绍"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 智能体进行策略学习（回合更新）\n",
    "    def learn(self, obs, action, reward):\n",
    "        \"\"\" 用policy gradient 算法更新policy model\n",
    "        \"\"\"\n",
    "        # 输入 观测obs张量（Tensor）到 神经网络模型（策略）\n",
    "        # 获取神经网络（策略）输出的 动作概率prob张量（Tensor）\n",
    "        prob = self.model(obs)\n",
    "\n",
    "        # paddle.squeeze将输入Tensor的Shape中尺寸为1的维度进行压缩\n",
    "        # 返回对维度进行压缩后的Tensor，数据类型与输入Tensor一致\n",
    "        action_onehot = paddle.squeeze(\n",
    "\n",
    "            # 将动作action 转换为 独热编码（one-hot encoding）\n",
    "            # 其中 num_classes 用于定义一个one-hot向量的长度\n",
    "            F.one_hot(action, num_classes=prob.shape[1]),\n",
    "\n",
    "            # axis 代表要压缩的轴，默认为None，表示对所有尺寸为1的维度进行压缩。\n",
    "            # 这边 输入 axis=1，表示对第1维数据维度为1的话，将进行压缩。\n",
    "            axis=1)\n",
    "\n",
    "        # 动作对数概率 = sum（对数动作概率（同策略神经网络输出的） * 动作的独热编码（one-hot encoding））\n",
    "        # 求和运算sum的维度 axis=-1，代表最后一维\n",
    "        log_prob = paddle.sum(paddle.log(prob) * action_onehot, axis=-1)\n",
    "\n",
    "        # 对 reward张量（Tensor）第1维数据维度为1的话，将进行压缩。\n",
    "        reward = paddle.squeeze(reward, axis=1)\n",
    "\n",
    "        # 对 每个动作对数概率 * 对应动作的未来累计总回报（奖励）\n",
    "        # 目标是 训练 使得 智能体的策略 能够 实现 每一步动作的未来累计总回报（奖励）最大化\n",
    "        # 而 Adam 神经网络参数优化器 是 梯度下降\n",
    "        # (-1 * ) 操作来实现 梯度上升\n",
    "        loss = paddle.mean(-1 * log_prob * reward)\n",
    "\n",
    "        # 清空梯度\n",
    "        self.optimizer.clear_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 参数更新\n",
    "        self.optimizer.step()\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "paddle.squeeze(x, axis=None, name=None)\n",
    "\n",
    "用途：\n",
    "会删除输入Tensor的Shape中尺寸为1的维度。如果指定了axis，则会删除axis中指定的尺寸为1的维度。如果没有指定axis，那么所有等于1的维度都会被删除。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "paddle.nn.functional.one_hot(x, num_classes, name=None)\n",
    "<br>\n",
    "用途：\n",
    "将输入'x'中的每个id转换为一个one-hot向量，其长度为 num_classes ，该id对应的向量维度上的值为1，其余维度的值为0。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### learn 函数 举例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作张量的独热编码的维度为： [3, 1, 3]\n",
      "动作张量的独热编码为： Tensor(shape=[3, 1, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\n",
      "       [[[0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1.]]])\n",
      "\n",
      "压缩后动作张量的独热编码的维度为： [3, 3]\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 动作概率prob，假设有 3 个 离散动作\n",
    "# 对应 在3个观测下，每个动作 的 概率，概率总和 为 1.0\n",
    "prob = paddle.to_tensor(\n",
    "    [\n",
    "        [0.4, 0.5, 0.1], # obs1\n",
    "        [0.2, 0.3, 0.5], # obs2\n",
    "        [0.6, 0.2, 0.2]  # obs3\n",
    "    ],\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "# 动作action，假设有 3 个 离散动作\n",
    "# 创建 action 动作数组，表示 在3个观测下，智能体 输出的 动作\n",
    "action = np.array([2, 0, 2])\n",
    "# 添加维度 ---> [action]\n",
    "action = np.expand_dims(action, axis=-1)\n",
    "# 将 动作action数组 转换为 Tensor张量，离散动作，以 整数 表示\n",
    "action = paddle.to_tensor(action, dtype='int32')\n",
    "\n",
    "# 将动作action 转换为 独热编码（one-hot encoding）\n",
    "# 其中 num_classes 用于定义一个one-hot向量的长度\n",
    "act_one_hot_encoding = F.one_hot(action, num_classes=prob.shape[1])\n",
    "print('动作张量的独热编码的维度为：', act_one_hot_encoding.shape)\n",
    "print('动作张量的独热编码为：', act_one_hot_encoding)\n",
    "\n",
    "# 将动作张量的独热编码Shape中第1维数据维度为1的话，将进行压缩。\n",
    "action_onehot = paddle.squeeze(act_one_hot_encoding, axis=1)\n",
    "print('\\n压缩后动作张量的独热编码的维度为：', action_onehot.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### learn 函数 对应公式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "代码：\n",
    "> log_prob = paddle.sum(paddle.log(prob) * action_onehot, axis=-1)\n",
    "\n",
    "对应公式：<br>\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla  logp_\\theta(\\tau )&=\\nabla\\sum_{t=1}^{T} logp_\\theta(a_t|s_t)\\\\&=\\sum_{t=1}^{T}\\nabla logp_\\theta(a_t|s_t)\n",
    "\\end{split}\n",
    "\\end{equation}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "代码：\n",
    "\n",
    "> loss = paddle.mean(-1 * log_prob * reward)\n",
    "\n",
    "对应公式：\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla \\bar{R_\\theta } = \\frac{1}{T} \\sum_{t=1}^{T} logp_\\theta (a_t|s_t) R(\\tau )\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "意思为：一个episode的轨迹内，平均每个步骤 **期望** 的未来累计总回报（奖励）。\n",
    "\n",
    "为使得智能体决策的每个步骤 **期望** 的未来累计总回报（奖励）最大化（优化目标）。<br>",
    "使用梯度上升。<br>",
    "对应公式：<br>",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "Loss&=-1*\\nabla \\bar{R_\\theta } \\\\&= -1*\\frac{1}{T} \\sum_{t=1}^{T} logp_\\theta (a_t|s_t) R(\\tau )\n",
    "\\end{split}\n",
    "\\end{equation}$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 参考链接\n",
    "- [Paddle Documentation](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/index_cn.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}