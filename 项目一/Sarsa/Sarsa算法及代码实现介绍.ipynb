{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 安装项目所需的第三方库"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install parl -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install visualdl -i https://mirror.baidu.com/pypi/simple\n",
    "!pip install gym==0.18.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 项目文件介绍\n",
    "\n",
    "agent.py  -----> 强化学习智能体（包含Sarsa算法）<br>\n",
    "gridworld.py -----> 游戏环境（包含悬崖环境）<br>\n",
    "train.py -----> 训练强化学习智能体 <br>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q表格示例\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ef57d8b92cb24cd4ae651892c99a00690a199689e06b4ebb9f742c313fab7f85)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sarsa 算法\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/77ca24ade3ef41f5a853fb64b6224e37590453181f4547b0859176231e4263dd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sarsa 算法 初始化\n",
    "``` python\n",
    "    def __init__(self,\n",
    "                     obs_n,\n",
    "                     act_n,\n",
    "                     learning_rate=0.01,\n",
    "                     gamma=0.9,\n",
    "                     e_greed=0.1):\n",
    "            self.act_n = act_n  # 动作维度，有几个动作可选\n",
    "            self.lr = learning_rate  # 学习率\n",
    "            self.gamma = gamma  # reward的衰减率\n",
    "            self.epsilon = e_greed  # 按一定概率随机选动作\n",
    "            self.Q = np.zeros((obs_n, act_n))  # 建立Q表格\n",
    "```\n",
    "其中：\n",
    "学习率 $\\alpha$ 默认为 0.01\n",
    "reward的衰减率（折扣因子）$\\gamma$ 默认为 0.9\n",
    "探索率 $\\varepsilon$ 默认为 0.1 ，即10%的概率进行探索（随机动作）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sarsa 算法 带探索的动作采样\n",
    "```Python\n",
    "    # 根据输入观察值，采样输出的动作值，带探索\n",
    "            def sample(self, obs):\n",
    "                if np.random.uniform(0, 1) < (1.0 - self.epsilon):  #根据table的Q值选动作\n",
    "                    action = self.predict(obs)\n",
    "                else:\n",
    "                    action = np.random.choice(self.act_n)  #有一定概率随机探索选取一个动作\n",
    "                return action\n",
    "```\n",
    "其中：\n",
    "使用 90% 的概率，从Q表格中选取当前 **状态$obs$** 的Q值最大的动作 （利用）\n",
    "使用 10% 的概率，从所有可选择的动作中，随机选择动作 （探索）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sarsa 算法 从策略（Q表格）中输出最优动作\n",
    "```Python\n",
    "    # 根据输入观察值，预测输出的动作值\n",
    "    def predict(self, obs):\n",
    "        Q_list = self.Q[obs, :]  # 获取状态obs下所有动作的Q值\n",
    "\n",
    "        maxQ = np.max(Q_list)  # 获取最大的Q值（maxQ）\n",
    "\n",
    "        action_list = np.where(Q_list == maxQ)[0]\n",
    "        # 获取最大Q值的索引位置（最优动作）\n",
    "        # maxQ可能对应多个action，意味着最优动作可能不只一个\n",
    "\n",
    "        action = np.random.choice(action_list)\n",
    "        # 选取最优动作，如果在最优动作不只一个的情况下，从中随机选择一个\n",
    "\n",
    "        return action\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sarsa 算法 策略更新\n",
    "```Python\n",
    "    # 学习方法，也就是更新Q-table的方法\n",
    "        def learn(self, obs, action, reward, next_obs, next_action, done):\n",
    "            \"\"\" on-policy\n",
    "                obs: 交互前的obs, s_t\n",
    "                action: 本次交互选择的action, a_t\n",
    "                reward: 本次动作获得的奖励r\n",
    "                next_obs: 本次交互后的obs, s_t+1\n",
    "                next_action: 根据当前Q表格, 针对next_obs会选择的动作, a_t+1\n",
    "                done: episode是否结束\n",
    "            \"\"\"\n",
    "            predict_Q = self.Q[obs, action]\n",
    "            if done:\n",
    "                target_Q = reward  # 没有下一个状态了\n",
    "            else:\n",
    "                target_Q = reward + self.gamma * self.Q[next_obs, next_action]  # Sarsa\n",
    "\n",
    "            self.Q[obs, action] += self.lr * (target_Q - predict_Q)  # 修正q\n",
    "```\n",
    "其中：\n",
    "> target_Q = reward + self.gamma * self.Q[next_obs, next_action]\n",
    "\n",
    "公式为：$Q_{target}=R+\\gamma Q(S_{t+1}, A_{t+1})$\n",
    "\n",
    "> self.Q[obs, action] += self.lr * (target_Q - predict_Q)\n",
    "\n",
    "公式为：$Q(S_t,A_t)=Q(S_t,A_t)+\\alpha (Q_{target}-Q(S_t,A_t))$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}